{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: force reinstallation of numpy to this version pip install numpy==1.23.5\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.23.5\n",
        "import numpy as np\n",
        "np.__version__\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "Y8ddIla5CUMF",
        "outputId": "18a09d2c-3770-4ee0-b308-95e5a1bd2b7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.0.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.35.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.3 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.20.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "597b76d1ee5d493e8a571515c626eecb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.23.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.vector import SyncVectorEnv\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# GPU Configuration\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Mixed precision policy\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "class EnhancedFishingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(5,), dtype=np.float32)\n",
        "        self.max_steps = 50\n",
        "        self.seed()\n",
        "        self.reset()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random = np.random.RandomState(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = 100.0\n",
        "        self.fishes = 10.0\n",
        "        self.step_count = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        terminated = False\n",
        "        truncated = self.step_count >= self.max_steps\n",
        "        reward = 0.0\n",
        "\n",
        "        if action == 0:  # Fish\n",
        "            if self.fishes > 0:\n",
        "                self.fishes -= 1\n",
        "                reward = 10 + self.balance * 0.01\n",
        "                self.balance += 10\n",
        "            else:\n",
        "                reward = -100\n",
        "        elif action == 1:  # Gamble\n",
        "            if self.balance > 0 and self.fishes > 0:\n",
        "                bet = min(self.balance, self.fishes * 10)\n",
        "                outcome = self.np_random.choice([1.8, 0], p=[0.47, 0.53])\n",
        "                self.balance += bet * outcome\n",
        "                reward = bet * outcome\n",
        "            else:\n",
        "                reward = -10\n",
        "        else:  # Quit\n",
        "            terminated = True\n",
        "\n",
        "        if self.balance <= 0:\n",
        "            terminated = True\n",
        "\n",
        "        return self._get_state(), reward / 100.0, terminated, truncated, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        return np.array([\n",
        "            self.balance / 1000.0 - 0.5,\n",
        "            self.fishes / 50.0 - 0.2,\n",
        "            self.step_count / self.max_steps,\n",
        "            (self.balance - 100) / 500.0,\n",
        "            np.log1p(self.fishes) / 4.0\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "class PrioritizedReplay:\n",
        "    def __init__(self, capacity=1000000):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
        "        self.pos = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, transition, priority=1.0):\n",
        "        priority = max(priority, 1e-8)  # Ensure priority is at least 1e-8\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(transition)\n",
        "        else:\n",
        "            self.buffer[self.pos] = transition\n",
        "\n",
        "        self.priorities[self.pos] = priority\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def sample(self, batch_size, alpha=0.6):\n",
        "        if self.size == 0:\n",
        "            return [], [], []\n",
        "\n",
        "        probs = self.priorities[:self.size] ** alpha\n",
        "        probs_sum = probs.sum()\n",
        "\n",
        "        if probs_sum <= 1e-8:\n",
        "            probs = np.ones_like(probs) / self.size\n",
        "        else:\n",
        "            probs /= probs_sum\n",
        "\n",
        "        probs = probs / probs.sum()  # Ensure probabilities sum to 1\n",
        "\n",
        "        indices = np.random.choice(self.size, batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "        weights = (self.size * probs[indices]) ** (-0.4)\n",
        "        weights /= weights.max() + 1e-8\n",
        "\n",
        "        return samples, indices, weights.astype(np.float32)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_size):\n",
        "        self.action_size = action_size\n",
        "        self.model = self._build_model(state_shape)\n",
        "        self.target_model = self._build_model(state_shape)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=10.0)\n",
        "        self.replay = PrioritizedReplay()\n",
        "        self.gamma = 0.99\n",
        "        self.batch_size = 8192\n",
        "        self.env = SyncVectorEnv([lambda: EnhancedFishingEnv() for _ in range(8)], new_step_api=True)\n",
        "        self.update_freq = 4\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Early stopping parameters\n",
        "        self.best_balance = 0\n",
        "        self.patience_counter = 0\n",
        "        self.patience_limit = 10  # Number of evaluations without improvement to wait\n",
        "        self.improvement_threshold = 0.01  # Minimum improvement considered significant\n",
        "\n",
        "    def _build_model(self, input_shape):\n",
        "        inputs = tf.keras.Input(shape=input_shape)\n",
        "        x = layers.Dense(512, activation='swish')(inputs)\n",
        "        x = layers.Dense(512, activation='swish')(x)\n",
        "        x = layers.Dense(256, activation='swish')(x)\n",
        "        outputs = layers.Dense(self.action_size)(x)\n",
        "        return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, states, actions, rewards, next_states, dones, weights):\n",
        "        with tf.GradientTape() as tape:\n",
        "            next_q = tf.cast(self.target_model(next_states), tf.float32)\n",
        "            gamma = tf.constant(self.gamma, dtype=tf.float32)\n",
        "            target_q = rewards + (1 - dones) * gamma * tf.reduce_max(next_q, axis=1)\n",
        "            current_q = tf.reduce_sum(\n",
        "                tf.cast(self.model(states), tf.float32) *\n",
        "                tf.one_hot(actions, self.action_size),\n",
        "                axis=1\n",
        "            )\n",
        "            td_error = tf.abs(target_q - current_q)\n",
        "            loss = tf.reduce_mean(weights * tf.keras.losses.huber(target_q, current_q))\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss, td_error\n",
        "\n",
        "    def learn(self):\n",
        "        if self.replay.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch, indices, weights = self.replay.sample(self.batch_size)\n",
        "        if not batch:\n",
        "            return\n",
        "\n",
        "        states = tf.convert_to_tensor([x[0] for x in batch], dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor([x[1] for x in batch], dtype=tf.int32)\n",
        "        rewards = tf.convert_to_tensor([x[2] for x in batch], dtype=tf.float32)\n",
        "        next_states = tf.convert_to_tensor([x[3] for x in batch], dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor([float(x[4]) for x in batch], dtype=tf.float32)\n",
        "        weights = tf.convert_to_tensor(weights, dtype=tf.float32)\n",
        "\n",
        "        loss, td_errors = self._train_step(states, actions, rewards, next_states, dones, weights)\n",
        "\n",
        "        # Update priorities\n",
        "        new_priorities = td_errors.numpy() + 1e-5\n",
        "        for idx, priority in zip(indices, new_priorities):\n",
        "            self.replay.priorities[idx] = priority\n",
        "\n",
        "    def train(self, total_steps=1_000_000):\n",
        "        stats = {'rewards': [], 'balances': [], 'steps': []}\n",
        "        states = self.env.reset()\n",
        "\n",
        "        while self.step_count < total_steps:\n",
        "            # Collect experiences using vectorized environment\n",
        "            q_values = self.model(tf.convert_to_tensor(states, dtype=tf.float32))\n",
        "            actions = tf.argmax(q_values, axis=1).numpy()\n",
        "            next_states, rewards, terminateds, truncateds, _ = self.env.step(actions)\n",
        "            dones = np.logical_or(terminateds, truncateds)\n",
        "\n",
        "            # Store experiences\n",
        "            for i in range(self.env.num_envs):\n",
        "                priority = max(abs(rewards[i]), 1e-8)\n",
        "                self.replay.add((\n",
        "                    states[i],\n",
        "                    actions[i],\n",
        "                    rewards[i],\n",
        "                    next_states[i],\n",
        "                    float(dones[i])\n",
        "                ), priority=priority)\n",
        "\n",
        "            states = next_states\n",
        "            self.step_count += self.env.num_envs\n",
        "\n",
        "            # Learn from experiences\n",
        "            if self.step_count % self.update_freq == 0:\n",
        "                self.learn()\n",
        "\n",
        "            # Evaluation and early stopping check\n",
        "            if self.step_count % 1000 == 0:\n",
        "                self.target_model.set_weights(self.model.get_weights())\n",
        "                avg_reward, avg_balance = self._evaluate()\n",
        "                stats['rewards'].append(avg_reward)\n",
        "                stats['balances'].append(avg_balance)\n",
        "                stats['steps'].append(self.step_count)\n",
        "\n",
        "                # Check for improvement\n",
        "                improvement = (avg_balance - self.best_balance) / self.best_balance if self.best_balance != 0 else float('inf')\n",
        "\n",
        "                if avg_balance > self.best_balance * (1 + self.improvement_threshold):\n",
        "                    self.best_balance = avg_balance\n",
        "                    self.patience_counter = 0  # Reset counter\n",
        "                    self.model.save(f\"best_model_{avg_balance:.0f}.keras\")\n",
        "                    print(f\"Step {self.step_count} | Avg Balance: {avg_balance:.2f} | Avg Reward: {avg_reward:.2f} (Improvement: {improvement*100:.2f}%)\")\n",
        "                else:\n",
        "                    self.patience_counter += 1\n",
        "                    print(f\"Step {self.step_count} | No improvement for {self.patience_counter}/{self.patience_limit} checks\")\n",
        "\n",
        "                # Early stopping condition\n",
        "                if self.patience_counter >= self.patience_limit:\n",
        "                    print(f\"\\nEarly stopping triggered! No improvement for {self.patience_limit} consecutive evaluations.\")\n",
        "                    print(f\"Final model saved as 'final_model_{self.best_balance:.0f}.keras'\")\n",
        "                    self.model.save(f\"final_model_{self.best_balance:.0f}.keras\")\n",
        "                    break\n",
        "\n",
        "        # Generate final graphs\n",
        "        self._plot_stats(stats)\n",
        "        self._plot_rolling_stats(stats)\n",
        "\n",
        "    def _evaluate(self, num_episodes=20):\n",
        "        env = EnhancedFishingEnv()\n",
        "        total_reward = 0\n",
        "        total_balance = 0\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = tf.argmax(self.model(tf.expand_dims(state, 0))[0]).numpy()\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "            total_reward += episode_reward\n",
        "            total_balance += env.balance\n",
        "\n",
        "        return total_reward/num_episodes, total_balance/num_episodes\n",
        "\n",
        "    def _plot_stats(self, stats):\n",
        "        plt.figure(figsize=(15,5))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(stats['steps'], stats['rewards'])\n",
        "        plt.title('Average Evaluation Reward')\n",
        "        plt.xlabel('Training Steps')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(stats['steps'], stats['balances'])\n",
        "        plt.title('Average Final Balance')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_progress.png')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_rolling_stats(self, stats):\n",
        "        # Calculate rolling averages for smoother trend visualization\n",
        "        window_size = max(1, len(stats['rewards']) // 20)\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Rolling reward\n",
        "        plt.subplot(1, 2, 1)\n",
        "        rolling_reward = pd.Series(stats['rewards']).rolling(window_size).mean()\n",
        "        plt.plot(stats['steps'], rolling_reward)\n",
        "        plt.title(f'Rolling Average Reward (Window: {window_size} evaluations)')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.ylabel('Smoothed Reward')\n",
        "\n",
        "        # Rolling balance\n",
        "        plt.subplot(1, 2, 2)\n",
        "        rolling_balance = pd.Series(stats['balances']).rolling(window_size).mean()\n",
        "        plt.plot(stats['steps'], rolling_balance)\n",
        "        plt.title(f'Rolling Average Balance (Window: {window_size} evaluations)')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.ylabel('Smoothed Balance')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('rolling_metrics.png')\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = DQNAgent((5,), 3)\n",
        "    agent.train()\n",
        "    print(\"Training complete! Check generated graphs:\")\n",
        "    print(\"- training_progress.png: Overall training metrics\")\n",
        "    print(\"- rolling_metrics.png: Smoothed trend analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycyDG50IBFzD",
        "outputId": "f94343ca-e031-4680-b6e7-d1dba2847862"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000 | Avg Balance: 120.00 | Avg Reward: 0.22 (Improvement: inf%)\n",
            "Step 2000 | No improvement for 1/10 checks\n",
            "Step 3000 | No improvement for 2/10 checks\n",
            "Step 4000 | No improvement for 3/10 checks\n",
            "Step 5000 | No improvement for 4/10 checks\n",
            "Step 6000 | No improvement for 5/10 checks\n",
            "Step 7000 | No improvement for 6/10 checks\n",
            "Step 8000 | No improvement for 7/10 checks\n",
            "Step 9000 | Avg Balance: 130.00 | Avg Reward: 0.33 (Improvement: 8.33%)\n",
            "Step 10000 | Avg Balance: 150.00 | Avg Reward: 0.56 (Improvement: 15.38%)\n",
            "Step 11000 | Avg Balance: 160.00 | Avg Reward: 0.68 (Improvement: 6.67%)\n",
            "Step 12000 | Avg Balance: 170.00 | Avg Reward: 0.79 (Improvement: 6.25%)\n",
            "Step 13000 | Avg Balance: 180.00 | Avg Reward: 0.91 (Improvement: 5.88%)\n",
            "Step 14000 | No improvement for 1/10 checks\n",
            "Step 15000 | Avg Balance: 837.20 | Avg Reward: 7.61 (Improvement: 365.11%)\n",
            "Step 16000 | Avg Balance: 4465.00 | Avg Reward: 43.65 (Improvement: 433.33%)\n",
            "Step 17000 | No improvement for 1/10 checks\n",
            "Step 18000 | No improvement for 2/10 checks\n",
            "Step 19000 | No improvement for 3/10 checks\n",
            "Step 20000 | No improvement for 4/10 checks\n",
            "Step 21000 | Avg Balance: 4546.00 | Avg Reward: 44.46 (Improvement: 1.81%)\n",
            "Step 22000 | No improvement for 1/10 checks\n",
            "Step 23000 | No improvement for 2/10 checks\n",
            "Step 24000 | No improvement for 3/10 checks\n",
            "Step 25000 | No improvement for 4/10 checks\n",
            "Step 26000 | No improvement for 5/10 checks\n",
            "Step 27000 | No improvement for 6/10 checks\n",
            "Step 28000 | No improvement for 7/10 checks\n",
            "Step 29000 | No improvement for 8/10 checks\n",
            "Step 30000 | No improvement for 9/10 checks\n",
            "Step 31000 | No improvement for 10/10 checks\n",
            "\n",
            "Early stopping triggered! No improvement for 10 consecutive evaluations.\n",
            "Final model saved as 'final_model_4546.keras'\n",
            "Training complete! Check generated graphs:\n",
            "- training_progress.png: Overall training metrics\n",
            "- rolling_metrics.png: Smoothed trend analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIB2-ID8JACw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}